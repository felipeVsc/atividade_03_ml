{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (32,) (32,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m output \u001b[39m=\u001b[39m softmax(z3)\n\u001b[1;32m     69\u001b[0m \u001b[39m# Cálculo do erro\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39msum(y_train[i:i\u001b[39m+\u001b[39;49mbatch_size] \u001b[39m*\u001b[39;49m np\u001b[39m.\u001b[39;49mlog(output)) \u001b[39m/\u001b[39m batch_size\n\u001b[1;32m     72\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[1;32m     73\u001b[0m grad_output \u001b[39m=\u001b[39m (output \u001b[39m-\u001b[39m y_train[i:i\u001b[39m+\u001b[39mbatch_size]) \u001b[39m/\u001b[39m batch_size\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/ops/common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     79\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/arraylike.py:202\u001b[0m, in \u001b[0;36mOpsMixin.__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__mul__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__mul__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_arith_method(other, operator\u001b[39m.\u001b[39;49mmul)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:6112\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_arith_method\u001b[39m(\u001b[39mself\u001b[39m, other, op):\n\u001b[1;32m   6111\u001b[0m     \u001b[39mself\u001b[39m, other \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39malign_method_SERIES(\u001b[39mself\u001b[39m, other)\n\u001b[0;32m-> 6112\u001b[0m     \u001b[39mreturn\u001b[39;00m base\u001b[39m.\u001b[39;49mIndexOpsMixin\u001b[39m.\u001b[39;49m_arith_method(\u001b[39mself\u001b[39;49m, other, op)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/base.py:1348\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1345\u001b[0m rvalues \u001b[39m=\u001b[39m ensure_wrapped_if_datetimelike(rvalues)\n\u001b[1;32m   1347\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1348\u001b[0m     result \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49marithmetic_op(lvalues, rvalues, op)\n\u001b[1;32m   1350\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(result, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:232\u001b[0m, in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    228\u001b[0m     _bool_arith_check(op, left, right)\n\u001b[1;32m    230\u001b[0m     \u001b[39m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     \u001b[39m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     res_values \u001b[39m=\u001b[39m _na_arithmetic_op(left, right, op)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39mreturn\u001b[39;00m res_values\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:171\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    168\u001b[0m     func \u001b[39m=\u001b[39m partial(expressions\u001b[39m.\u001b[39mevaluate, op)\n\u001b[1;32m    170\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     result \u001b[39m=\u001b[39m func(left, right)\n\u001b[1;32m    172\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_cmp \u001b[39mand\u001b[39;00m (is_object_dtype(left\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m is_object_dtype(right)):\n\u001b[1;32m    174\u001b[0m         \u001b[39m# For object dtype, fallback to a masked operation (only operating\u001b[39;00m\n\u001b[1;32m    175\u001b[0m         \u001b[39m#  on the non-missing values)\u001b[39;00m\n\u001b[1;32m    176\u001b[0m         \u001b[39m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[1;32m    177\u001b[0m         \u001b[39m#  incorrectly, see GH#32047\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:239\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m op_str \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m use_numexpr:\n\u001b[1;32m    238\u001b[0m         \u001b[39m# error: \"None\" not callable\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m         \u001b[39mreturn\u001b[39;00m _evaluate(op, op_str, a, b)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:70\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m _TEST_MODE:\n\u001b[1;32m     69\u001b[0m     _store_test_result(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 70\u001b[0m \u001b[39mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (32,) (32,3) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Carregar o conjunto de dados Abalone (substitua esta parte pelo carregamento real do seu conjunto de dados)\n",
    "# X, y = load_abalone_data()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('abalone_dataset.csv')\n",
    "\n",
    "X = pd.read_csv('abalone_dataset.csv')\n",
    "y = X.type\n",
    "X = X.drop(columns=['type','sex'])\n",
    "\n",
    "# Exemplo de geração de dados fictícios para demonstração\n",
    "# X = np.random.rand(1000, 8)  # 1000 amostras, 8 características\n",
    "# y = np.random.randint(0, 3, size=1000)  # Classes 0, 1, 2\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "# y_onehot = np.eye(3)[y]  # Transformar as classes em codificação one-hot\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Função de ativação ReLU\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Função de ativação Softmax\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Inicialização dos parâmetros da rede neural\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "output_size = 3\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Inicialização dos pesos e bias\n",
    "weights1 = np.random.randn(input_size, hidden_size1)\n",
    "bias1 = np.zeros((1, hidden_size1))\n",
    "weights2 = np.random.randn(hidden_size1, hidden_size2)\n",
    "bias2 = np.zeros((1, hidden_size2))\n",
    "weights3 = np.random.randn(hidden_size2, output_size)\n",
    "bias3 = np.zeros((1, output_size))\n",
    "\n",
    "# Treinamento da rede neural\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        # Forward pass\n",
    "        z1 = np.dot(X_train[i:i+batch_size], weights1) + bias1\n",
    "        a1 = relu(z1)\n",
    "        z2 = np.dot(a1, weights2) + bias2\n",
    "        a2 = relu(z2)\n",
    "        z3 = np.dot(a2, weights3) + bias3\n",
    "        output = softmax(z3)\n",
    "        \n",
    "        # Cálculo do erro\n",
    "        loss = -np.sum(y_train[i:i+batch_size] * np.log(output)) / batch_size\n",
    "        \n",
    "        # Backpropagation\n",
    "        grad_output = (output - y_train[i:i+batch_size]) / batch_size\n",
    "        grad_weights3 = np.dot(a2.T, grad_output)\n",
    "        grad_bias3 = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        grad_a2 = np.dot(grad_output, weights3.T)\n",
    "        grad_z2 = grad_a2 * (a2 > 0)\n",
    "        grad_weights2 = np.dot(a1.T, grad_z2)\n",
    "        grad_bias2 = np.sum(grad_z2, axis=0, keepdims=True)\n",
    "        grad_a1 = np.dot(grad_z2, weights2.T)\n",
    "        grad_z1 = grad_a1 * (a1 > 0)\n",
    "        grad_weights1 = np.dot(X_train[i:i+batch_size].T, grad_z1)\n",
    "        grad_bias1 = np.sum(grad_z1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Atualização dos pesos e bias\n",
    "        weights3 -= learning_rate * grad_weights3\n",
    "        bias3 -= learning_rate * grad_bias3\n",
    "        weights2 -= learning_rate * grad_weights2\n",
    "        bias2 -= learning_rate * grad_bias2\n",
    "        weights1 -= learning_rate * grad_weights1\n",
    "        bias1 -= learning_rate * grad_bias1\n",
    "        \n",
    "    # Calcular a acurácia no final de cada época\n",
    "    z1 = np.dot(X_test, weights1) + bias1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, weights2) + bias2\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(a2, weights3) + bias3\n",
    "    predicted_probs = softmax(z3)\n",
    "    predicted_labels = np.argmax(predicted_probs, axis=1)\n",
    "    true_labels = np.argmax(y_test, axis=1)\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "## Avaliação final\n",
    "z1 = np.dot(X_test, weights1) + bias1\n",
    "a1 = relu(z1)\n",
    "z2 = np.dot(a1, weights2) + bias2\n",
    "a2 = relu(z2)\n",
    "z3 = np.dot(a2, weights3) + bias3\n",
    "predicted_probs = softmax(z3)\n",
    "predicted_labels = np.argmax(predicted_probs, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"Final Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Cálculo da acurácia usando y_pred\n",
    "y_pred = predicted_labels\n",
    "y_true = true_labels\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Final Accuracy (using y_pred): {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy (using y_pred): 0.6298\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression com L2\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('abalone_dataset.csv')\n",
    "\n",
    "X = pd.read_csv('abalone_dataset.csv')\n",
    "y = X.type\n",
    "X = X.drop(columns=['type','sex'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(random_state=0,multi_class='multinomial').fit(X_train, y_train)\n",
    "res = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, res)\n",
    "print(f\"Final Accuracy (using y_pred): {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy (using y_pred): 0.6489\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression sem regularization\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('abalone_dataset.csv')\n",
    "\n",
    "X = pd.read_csv('abalone_dataset.csv')\n",
    "y = X.type\n",
    "X = X.drop(columns=['type','sex'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(random_state=0,penalty=None,multi_class='multinomial',max_iter=100000).fit(X_train, y_train)\n",
    "res = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, res)\n",
    "print(f\"Final Accuracy (using y_pred): {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.665389527458493"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('abalone_dataset.csv')\n",
    "\n",
    "X = pd.read_csv('abalone_dataset.csv')\n",
    "y = X.type\n",
    "X = X.drop(columns=['type','sex'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=1)\n",
    "clf = MLPClassifier(random_state=1, max_iter=1000000, learning_rate_init=0.01).fit(X_train, y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6679438058748404"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mudando a quantidade de hidden layers\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('abalone_dataset.csv')\n",
    "X = pd.read_csv('abalone_dataset.csv')\n",
    "y = X.type\n",
    "X = X.drop(columns=['type','sex'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=1)\n",
    "clf = MLPClassifier(random_state=1, hidden_layer_sizes=(5,),max_iter=1000000, learning_rate_init=0.01).fit(X_train, y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor 0: 0 ocorrências\n",
      "Valor 1: 1078 ocorrências\n",
      "Valor 2: 1003 ocorrências\n",
      "Valor 3: 1051 ocorrências\n"
     ]
    }
   ],
   "source": [
    "# fazer o mlp na mão com os codigos de deep learning para ficar melhor de explicar\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('abalone_dataset.csv')\n",
    "\n",
    "X = pd.read_csv('abalone_dataset.csv')\n",
    "y = X.type\n",
    "X = X.drop(columns=['type','sex'])\n",
    "value_counts = np.bincount(y)\n",
    "for value, count in enumerate(value_counts):\n",
    "    print(f\"Valor {value}: {count} ocorrências\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
